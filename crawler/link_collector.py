import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin  # URL íŒŒì‹±ì„ ìœ„í•¨

# --- âš™ï¸ ì„¤ì •: ì—¬ê¸°ì— ìŠ¤í¬ë˜í•‘ ê·œì¹™ì„ ì •ì˜í•˜ì„¸ìš” ---
#
# ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ì˜ LNB(ì¢Œì¸¡ ë©”ë‰´) êµ¬ì¡°ì— ëŒ€ì‘í•  ìˆ˜ ìˆë„ë¡
# ê·œì¹™ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤.
#
# 'name':         ê·œì¹™ì˜ ì´ë¦„ (ë¡œê·¸ì— í‘œì‹œë¨)
# 'main_selector': 1ë‹¨ê³„ ë©”ì¸ ë©”ë‰´(ìƒìœ„ ì¹´í…Œê³ ë¦¬) ë§í¬ë¥¼ ì°¾ëŠ” CSS ì„ íƒì
# 'sub_selector':  ê° 1ë‹¨ê³„ ë©”ë‰´ í˜ì´ì§€ì— ë°©ë¬¸í–ˆì„ ë•Œ,
#                  í™œì„±í™”ëœ('on' ë˜ëŠ” 'active') í•˜ìœ„ ë©”ë‰´ ë§í¬ë¥¼ ì°¾ëŠ” CSS ì„ íƒì
#
CRAWL_RULES = [
    {
        "name": "ë™ì‘êµ¬ ê±´ê°•ê´€ë¦¬ì²­ LNB",
        "main_selector": ".left-area .left-mdp1 > li > a",
        "policy_finders": [
            ".left-mdp1 > li.on > ul > li > a",  # 1ìˆœìœ„: LNB í•˜ìœ„ ë©”ë‰´ (ì˜ˆ: ì˜ìœ ì•„Â·ëª¨ì„±)
            ".nw-tab-bx .nw-tab-ls > li > p > a",  # 2ìˆœìœ„: ë³¸ë¬¸ íƒ­ ë©”ë‰´ (ì˜ˆ: ì¹˜ë§¤ê´€ë¦¬)
        ],
    },
    {
        "name": "ë‹¤ë¥¸ ì‚¬ì´íŠ¸ ì˜ˆì‹œ (ê°€ìƒ LNB)",
        "main_selector": ".lnb_menu > ul > li > a",
        "policy_finders": [
            ".lnb_menu > ul > li.active > ul > li > a",
            ".content_tabs > .tab_list > a",
        ],
    },
    # ì—¬ê¸°ì— ë‹¤ë¥¸ ì‚¬ì´íŠ¸ì˜ ê·œì¹™ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
]
# --- ì„¤ì • ë ---


def get_soup(url, session):
    """
    ì£¼ì–´ì§„ URLì— ì ‘ì†í•˜ì—¬ BeautifulSoup ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    HTTP ì„¸ì…˜(session)ì„ ì‚¬ìš©í•˜ì—¬ ì—°ê²°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = session.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # 200 OKê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜ ë°œìƒ
        return BeautifulSoup(response.text, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"  [ì˜¤ë¥˜] {url} ì ‘ì† ì‹¤íŒ¨: {e}")
        return None


def main():
    """
    ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
    """
    # 1. ì‚¬ìš©ì ì…ë ¥
    start_url = input("ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")

    # 2. â­ ë™ì  base_url ìƒì„±
    # ì˜ˆ: https://example.com/page/a -> https://example.com
    try:
        parsed_url = urlparse(start_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        if not parsed_url.scheme or not parsed_url.netloc:
            raise ValueError
        print(f"--- 0ë‹¨ê³„: ê¸°ë³¸ URLì„ '{base_url}' (ìœ¼)ë¡œ ì„¤ì •í•©ë‹ˆë‹¤ ---")
    except ValueError:
        print(
            "[ì˜¤ë¥˜] ìœ íš¨í•˜ì§€ ì•Šì€ URLì…ë‹ˆë‹¤. 'http://' ë˜ëŠ” 'https://'ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."
        )
        return

    # HTTP ì„¸ì…˜ ìƒì„±
    session = requests.Session()

    print("\n--- 1ë‹¨ê³„: ë©”ì¸ ì¹´í…Œê³ ë¦¬ ë§í¬ ìˆ˜ì§‘ ì‹œì‘ ---")

    # 3. ì‹œì‘ í˜ì´ì§€ íŒŒì‹±
    soup = get_soup(start_url, session)
    if not soup:
        print("ì‹œì‘ í˜ì´ì§€ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 4. â­ ì¼ì¹˜í•˜ëŠ” 'ê·œì¹™' ì°¾ê¸°
    main_links = []
    active_rule = None

    for rule in CRAWL_RULES:
        print(f"  [ì‹œë„] ê·œì¹™ '{rule['name']}' (ì„ íƒì: {rule['main_selector']})")
        main_links = soup.select(rule["main_selector"])
        if main_links:
            print(f"  [ì„±ê³µ] ì´ ê·œì¹™ìœ¼ë¡œ {len(main_links)}ê°œì˜ ë§í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            active_rule = rule  # ì‚¬ìš©ëœ ê·œì¹™ì„ ì €ì¥
            break

    if not active_rule:
        print("\n[ì˜¤ë¥˜] 1ë‹¨ê³„ ë©”ë‰´ ë§í¬ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("CRAWL_RULESì— ì •ì˜ëœ 'main_selector' ì¤‘ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 5. 1ë‹¨ê³„ ë©”ë‰´ ë§í¬ ì²˜ë¦¬
    main_categories = []
    for link in main_links:
        category_name = link.get_text().strip()
        relative_href = link.get("href")

        # ìƒëŒ€ ê²½ë¡œ(/...)ë¥¼ ì ˆëŒ€ ê²½ë¡œ(https://...)ë¡œ ë³€í™˜
        absolute_url = urljoin(base_url, relative_href)

        # ì™¸ë¶€ ë§í¬(http...)ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if relative_href.startswith("http"):
            absolute_url = relative_href

        main_categories.append({"name": category_name, "url": absolute_url})
        # print(f"  [ìˆ˜ì§‘] {category_name} ({absolute_url})") # 1ë‹¨ê³„ ë¡œê·¸ëŠ” ì„±ê³µ ë¡œê·¸ë¡œ ëŒ€ì²´

    print(
        f"\n--- 2ë‹¨ê³„: ì´ {len(main_categories)}ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœíšŒí•˜ë©° í•˜ìœ„ ë©”ë‰´ ìˆ˜ì§‘ ---"
    )

    # 6. ìˆ˜ì§‘ëœ 1ë‹¨ê³„ ë©”ë‰´ë¥¼ ìˆœíšŒí•˜ë©° ê° í˜ì´ì§€ì˜ í•˜ìœ„ ë©”ë‰´ ìˆ˜ì§‘
    all_menus_data = {}

    for category in main_categories:
        print(f"\n[ë°©ë¬¸ ì¤‘...] {category['name']} ({category['url']})")

        # ì™¸ë¶€ ë§í¬(base_urlë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ë§í¬)ëŠ” ê±´ë„ˆë›°ê¸°
        if not category["url"].startswith(base_url):
            print("  [ì•Œë¦¼] ì™¸ë¶€ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
            all_menus_data[category["name"]] = []
            continue

        category_soup = get_soup(category["url"], session)
        if not category_soup:
            continue

        sub_menu_list = []

        # â­ í™œì„±í™”ëœ ê·œì¹™(active_rule)ì˜ 'sub_selector'ë¥¼ ì‚¬ìš©
        found_sub_links = False

        for finder_selector in active_rule["policy_finders"]:
            sub_links = category_soup.select(finder_selector)

            if sub_links:
                # í•˜ìœ„ ë©”ë‰´ê°€ ìˆìœ¼ë©´(Case 1: LNB ë˜ëŠ” Tab), í•˜ìœ„ ë©”ë‰´ë“¤ì„ ìˆ˜ì§‘
                print(
                    f"  [ì•Œë¦¼] (ê·œì¹™: {finder_selector})ì—ì„œ í•˜ìœ„ ë©”ë‰´ {len(sub_links)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                )
                found_sub_links = True

                for sub_link in sub_links:
                    sub_name = sub_link.get_text().strip()
                    sub_href = urljoin(base_url, sub_link.get("href"))
                    sub_menu_list.append({"name": sub_name, "url": sub_href})

                break  # í•˜ìœ„ ë§í¬ë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë‹¤ìŒ ê·œì¹™(finder)ì€ í™•ì¸í•  í•„ìš” ì—†ìŒ

        if not found_sub_links:
            # í•˜ìœ„ ë©”ë‰´ê°€ ì—†ìœ¼ë©´(Case 2), ì¹´í…Œê³ ë¦¬ ìì²´ë¥¼ ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ê°„ì£¼
            print("  [ì•Œë¦¼] í•˜ìœ„ ë©”ë‰´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ ìì²´ë¥¼ í•­ëª©ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            sub_menu_list.append({"name": category["name"], "url": category["url"]})
        all_menus_data[category["name"]] = sub_menu_list

    # 7. ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n\n--- ğŸŒŸ ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼ ğŸŒŸ ---")
    for main_name, sub_menus in all_menus_data.items():
        print(f"\nâ–  {main_name}")
        if sub_menus:
            for sub in sub_menus:
                print(f"  - {sub['name']} ({sub['url']})")
        else:
            print("  (í•˜ìœ„ ë©”ë‰´ ì—†ìŒ ë˜ëŠ” ì™¸ë¶€ ë§í¬)")


# --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    main()

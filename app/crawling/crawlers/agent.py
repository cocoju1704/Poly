# -*- coding: utf-8 -*-
import os
import asyncio
from typing import Optional
from dotenv import load_dotenv

from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.tools import tool
from langchain_community.vectorstores import FAISS
from langchain.agents import create_openai_tools_agent, AgentExecutor
from langchain_core.documents import Document

# =========================
# 0) í™˜ê²½ ì„¤ì •
# =========================
load_dotenv()
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
TEMP = float(os.getenv("TEMPERATURE", "0.2"))

RAW_PATH = "rawtext.txt"
if not os.path.exists(RAW_PATH):
    raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— rawtext.txtê°€ í•„ìš”í•©ë‹ˆë‹¤.")

with open(RAW_PATH, "r", encoding="utf-8") as f:
    raw_text = f.read()


# =========================
# 1) LLM ê¸°ë°˜ êµ¬ì¡°í™” ì¶”ì¶œ
# =========================
class NoticeInfo(BaseModel):
    """ê³µê³ ë¬¸ì—ì„œ ë°˜ë“œì‹œ ì¶”ì¶œí•´ì•¼ í•˜ëŠ” í•„ë“œ ì •ì˜"""

    title: str = Field(description="ê³µê³ /ì‚¬ì—…/í”„ë¡œê·¸ë¨ì˜ ì œëª©(í•œ ì¤„)")
    eligibility: str = Field(description="ì§€ì› ëŒ€ìƒ ë˜ëŠ” ì‹ ì²­/ì°¸ê°€ ìê²©ì„ ê°„ê²°íˆ ìš”ì•½")
    support: str = Field(description="ì§€ì› ë‚´ìš©/í˜œíƒ/ì§€ì› í•­ëª©ì„ í•µì‹¬ë§Œ ìš”ì•½")
    confidence: Optional[float] = Field(
        default=None, description="ì¶”ì¶œ ì‹ ë¢°ë„(0~1). í™•ì‹ ì´ ì—†ìœ¼ë©´ 0.4 ì´í•˜ë¡œ ì„¤ì •"
    )


def extract_structured_info_llm(text: str) -> NoticeInfo:
    """
    LLMì´ rawtextì—ì„œ í•„ìˆ˜ í•„ë“œë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜.
    - LangChain structured output(Pydantic)ì„ ì‚¬ìš©í•´ JSON ìŠ¤í‚¤ë§ˆë¡œ ê°•ì œ.
    - ë‚´ìš©ì´ ë¶€ì¡±í•˜ë©´ 'ì›ë¬¸ì— ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ì±„ìš°ë„ë¡ ì§€ì‹œ.
    """
    extractor_llm = ChatOpenAI(model=MODEL, temperature=0)  # ì¶”ì¶œì€ ê²°ì •ì ì´ê²Œ
    structured = extractor_llm.with_structured_output(NoticeInfo)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë„ˆëŠ” í•œêµ­ì–´ ê³µê³ ë¬¸ì„ êµ¬ì¡°ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë³´ì¡°ìì•¼.
ë‹¤ìŒ ì›ë¬¸ì—ì„œ 'ì œëª©', 'ì§€ì› ëŒ€ìƒ(ìê²©)', 'ì§€ì› ë‚´ìš©'ì„ ê¼­ ë½‘ì•„.
ê·œì¹™:
- ì›ë¬¸ì— ê·¼ê±°í•´ ì‘ì„±í•˜ê³ , ì—†ìœ¼ë©´ 'ì›ë¬¸ì— ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ì ì–´.
- ì œëª©ì€ í•œ ì¤„ë¡œ ìš”ì•½.
- ì§€ì› ëŒ€ìƒê³¼ ì§€ì› ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½ (ê¸¸ì–´ë„ 4~6ì¤„ ì´ë‚´).
- í¬ë§·ì€ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ(NoticeInfo)ì— ë§ì¶° ë°˜í™˜.""",
            ),
            (
                "human",
                "ë‹¤ìŒ ì›ë¬¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜:\n\n================ RAW TEXT ================\n{raw}\n=========================================",
            ),
        ]
    )

    chain = prompt | structured
    return chain.invoke({"raw": text})


extracted = extract_structured_info_llm(raw_text)

# =========================
# 2) ì¸ë±ì‹±(ì¸ë©”ëª¨ë¦¬ FAISS)
# =========================
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800, chunk_overlap=120, separators=["\n\n", "\n", " ", ""]
)
docs = [Document(page_content=raw_text, metadata={"source": "rawtext.txt"})]
chunks = splitter.split_documents(docs)

embeddings = OpenAIEmbeddings()  # text-embedding-3-small ê¸°ë³¸
vector_store = FAISS.from_documents(chunks, embeddings)


# =========================
# 3) ê²€ìƒ‰ ë„êµ¬ (@tool)
# =========================
@tool
def search_documents(query: str) -> str:
    """
    ì¸ë©”ëª¨ë¦¬ FAISSì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë³¸ë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        results = vector_store.similarity_search(query, k=5)
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        out = []
        for i, doc in enumerate(results, start=1):
            out.append(
                f"[ë¬¸ì„œ {i} | ì¶œì²˜: {doc.metadata.get('source', 'rawtext.txt')}]\n{doc.page_content}\n"
            )
        return "\n".join(out)
    except Exception as e:
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


@tool
def search_documents_with_score(query: str) -> str:
    """
    ì¸ë©”ëª¨ë¦¬ FAISSì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        results = vector_store.similarity_search_with_score(query, k=5)
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        out = []
        for i, (doc, score) in enumerate(results, start=1):
            preview = doc.page_content[:500].replace("\n", " ")
            out.append(
                f"[ë¬¸ì„œ {i} | ì ìˆ˜: {score:.4f} | ì¶œì²˜: {doc.metadata.get('source', 'rawtext.txt')}] {preview}..."
            )
        return "\n".join(out)
    except Exception as e:
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


tools = [search_documents, search_documents_with_score]

# =========================
# 4) í”„ë¡¬í”„íŠ¸ & ì—ì´ì „íŠ¸
# =========================
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œê³µí•œ rawtext.txtë§Œì„ ê·¼ê±°ë¡œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
- ë°˜ë“œì‹œ ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼(íˆ´ ì¶œë ¥) ë²”ìœ„ ì•ˆì—ì„œë§Œ ë‹µí•˜ì„¸ìš”.
- ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ 'ì›ë¬¸ì— ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
- í•µì‹¬ ìš”ì ì„ ì§§ê²Œ bulletë¡œ ì •ë¦¬í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
- ë‹µë³€ ëì— ê°„ë‹¨íˆ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”. (ì˜ˆ: ì¶œì²˜: rawtext.txt)
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_PROMPT),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

llm = ChatOpenAI(model=MODEL, temperature=TEMP, streaming=True)
agent = create_openai_tools_agent(llm, tools, prompt)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,
    max_iterations=5,
)


# =========================
# 5) ë©€í‹°í„´ + ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
# =========================
def print_pre_extracted_info_llm(n: NoticeInfo) -> None:
    print("=" * 70)
    print("ğŸ¤– LLM ì‚¬ì „ ì¶”ì¶œ ì •ë³´ (rawtext ê¸°ë°˜)")
    print("=" * 70)
    print(f"â–  ì œëª©\n{n.title}\n")
    print(f"â–  ì§€ì› ëŒ€ìƒ(ìê²©)\n{n.eligibility}\n")
    print(f"â–  ì§€ì› ë‚´ìš©\n{n.support}\n")
    if n.confidence is not None:
        print(f"â–  ì‹ ë¢°ë„(LLM ìì²´ ì¶”ì •): {n.confidence:.2f}\n")
    print("-" * 70)


async def run_multiturn_conversation():
    chat_history = []

    # LLM ì¶”ì¶œ ê²°ê³¼ ë¨¼ì € ì¶œë ¥
    print_pre_extracted_info_llm(extracted)

    print("Agentic RAG (rawtext.txt / ì¸ë©”ëª¨ë¦¬ FAISS)")
    print("=" * 70)
    print("ì¢…ë£Œ: quit/exit/ì¢…ë£Œ | ì´ˆê¸°í™”: reset/clear/ì´ˆê¸°í™”")

    while True:
        user_input = await asyncio.to_thread(input, "\nì§ˆë¬¸: ")
        if user_input is None:
            continue
        user_input = user_input.strip()

        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì‹œìŠ¤í…œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if user_input.lower() in ["reset", "clear", "ì´ˆê¸°í™”"]:
            chat_history = []
            print("ëŒ€í™” ë‚´ìš©ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print_pre_extracted_info_llm(extracted)
            continue
        if not user_input:
            continue

        try:
            print("ë‹µë³€: ", end="", flush=True)
            full_response = ""
            async for event in agent_executor.astream_events(
                {"input": user_input, "chat_history": chat_history},
                version="v2",
            ):
                kind = event["event"]
                if kind == "on_tool_start":
                    tool_name = event["name"]
                    print(f"\n[{tool_name}] ê²€ìƒ‰ ì¤‘...", end="", flush=True)
                    print(" ì™„ë£Œ\në‹µë³€: ", end="", flush=True)
                elif kind == "on_chat_model_stream":
                    chunk = event["data"]["chunk"].content
                    if chunk:
                        print(chunk, end="", flush=True)
                        full_response += chunk
            print()

            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(AIMessage(content=full_response))

        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")


# =========================
# 6) ì‹¤í–‰
# =========================
if __name__ == "__main__":
    asyncio.run(run_multiturn_conversation())

"""
ì›Œí¬í”Œë¡œìš°: ë§í¬ ìˆ˜ì§‘ â†’ í¬ë¡¤ë§ ë° êµ¬ì¡°í™”

1. crawler/link_collector.pyë¡œ ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ì˜ ëª¨ë“  ì„œë¸Œ ë©”ë‰´ ë§í¬ ìˆ˜ì§‘
2. ìˆ˜ì§‘ëœ ê° ë§í¬ë¥¼ crawler/llm_structured_crawler.pyë¡œ í¬ë¡¤ë§í•˜ì—¬ êµ¬ì¡°í™”
3. ëª¨ë“  ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
"""

import json
import os
import sys
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import uuid

# crawler í´ë”ì˜ ëª¨ë“ˆ import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "crawler"))
from llm_structured_crawler import LLMStructuredCrawler


class HealthCareWorkflow:
    """ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì›Œí¬í”Œë¡œìš°"""

    def __init__(self, output_dir: str = "output", region: str = None):
        """
        Args:
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            region: ì§€ì—­ëª… (ì˜ˆ: "ë™ì‘êµ¬"). Noneì´ë©´ URLì—ì„œ ìë™ ì¶”ì¶œ ì‹œë„
        """
        self.output_dir = output_dir
        self.region = region
        self.crawler = LLMStructuredCrawler(model="gpt-4o-mini")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)

    def extract_region_from_url(self, url: str) -> str:
        """
        URLì—ì„œ ì§€ì—­ëª… ì¶”ì¶œ ì‹œë„

        Args:
            url: ì†ŒìŠ¤ URL

        Returns:
            ì¶”ì¶œëœ ì§€ì—­ëª… (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ë„ë©”ì¸ëª…)
        """
        # ì§€ì—­ ë„ë©”ì¸ ë§¤í•‘ (í™•ì¥ ê°€ëŠ¥)
        region_mapping = {
            "dongjak": "ë™ì‘êµ¬",
            "gangnam": "ê°•ë‚¨êµ¬",
            "seocho": "ì„œì´ˆêµ¬",
            "songpa": "ì†¡íŒŒêµ¬",
            # í•„ìš”ì— ë”°ë¼ ì¶”ê°€
        }

        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        for key, value in region_mapping.items():
            if key in domain:
                return value

        # ë§¤í•‘ì— ì—†ìœ¼ë©´ ë„ë©”ì¸ì˜ ì²« ë¶€ë¶„ ë°˜í™˜
        return domain.split(".")[0]

    def collect_links(self, start_url: str, crawl_rules: List[Dict]) -> List[Dict]:
        """
        link_collector.pyì˜ ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ ë§í¬ ìˆ˜ì§‘

        Args:
            start_url: ì‹œì‘ URL
            crawl_rules: í¬ë¡¤ë§ ê·œì¹™ ë¦¬ìŠ¤íŠ¸

        Returns:
            ìˆ˜ì§‘ëœ ë§í¬ ë¦¬ìŠ¤íŠ¸ [{'name': '...', 'url': '...'}]
        """
        # base_url ìƒì„±
        parsed_url = urlparse(start_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"

        session = requests.Session()
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        # ì‹œì‘ í˜ì´ì§€ íŒŒì‹±
        response = session.get(start_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        # ê·œì¹™ ì°¾ê¸°
        main_links = []
        active_rule = None

        for rule in crawl_rules:
            main_links = soup.select(rule["main_selector"])
            if main_links:
                print(
                    f"  âœ“ ê·œì¹™ ì ìš©: '{rule['name']}' ({len(main_links)}ê°œ ë§í¬ ë°œê²¬)"
                )
                active_rule = rule
                break

        if not active_rule:
            raise ValueError("ì ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ê·œì¹™ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 1ë‹¨ê³„ ë©”ë‰´ ë§í¬ ì²˜ë¦¬
        main_categories = []
        for link in main_links:
            category_name = link.get_text().strip()
            relative_href = link.get("href")
            absolute_url = urljoin(base_url, relative_href)

            if relative_href.startswith("http"):
                absolute_url = relative_href

            main_categories.append({"name": category_name, "url": absolute_url})

        # 2ë‹¨ê³„: ê° ì¹´í…Œê³ ë¦¬ì˜ í•˜ìœ„ ë©”ë‰´ ìˆ˜ì§‘
        all_links = []

        for category in main_categories:
            print(f"\n  ë°©ë¬¸: {category['name']}")

            # ì™¸ë¶€ ë§í¬ ê±´ë„ˆë›°ê¸°
            if not category["url"].startswith(base_url):
                print("    â†’ ì™¸ë¶€ ë§í¬, ê±´ë„ˆëœ€")
                continue

            try:
                category_response = session.get(
                    category["url"], headers=headers, timeout=10
                )
                category_soup = BeautifulSoup(category_response.text, "html.parser")

                # í•˜ìœ„ ë©”ë‰´ ì°¾ê¸°
                sub_links = category_soup.select(active_rule["sub_selector"])

                if sub_links:
                    print(f"    â†’ í•˜ìœ„ ë©”ë‰´ {len(sub_links)}ê°œ ë°œê²¬")
                    for sub_link in sub_links:
                        sub_name = sub_link.get_text().strip()
                        sub_href = urljoin(base_url, sub_link.get("href"))
                        all_links.append({"name": sub_name, "url": sub_href})
                else:
                    # í•˜ìœ„ ë©”ë‰´ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ ìì²´ ì¶”ê°€
                    print("    â†’ í•˜ìœ„ ë©”ë‰´ ì—†ìŒ, ì¹´í…Œê³ ë¦¬ ìì²´ ì¶”ê°€")
                    all_links.append({"name": category["name"], "url": category["url"]})

            except Exception as e:
                print(f"    âœ— ì˜¤ë¥˜: {e}")
                continue

        return all_links

    def run(
        self,
        start_url: str,
        crawl_rules: List[Dict] = None,
        save_links: bool = True,
    ) -> Dict:
        """
        ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

        Args:
            start_url: ì‹œì‘ URL (ë³´ê±´ì†Œ ë³´ê±´ì‚¬ì—… í˜ì´ì§€)
            crawl_rules: í¬ë¡¤ë§ ê·œì¹™ ë¦¬ìŠ¤íŠ¸
            save_links: ìˆ˜ì§‘í•œ ë§í¬ë¥¼ JSONìœ¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€

        Returns:
            ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
        """
        print("=" * 80)
        print("ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
        print("=" * 80)

        # ê¸°ë³¸ í¬ë¡¤ë§ ê·œì¹™
        if crawl_rules is None:
            crawl_rules = [
                {
                    "name": "ë™ì‘êµ¬ ê±´ê°•ê´€ë¦¬ì²­ LNB",
                    "main_selector": ".left-area .left-mdp1 > li > a",
                    "sub_selector": ".left-mdp1 > li.on > ul > li > a",
                },
            ]

        # 1ë‹¨ê³„: ë§í¬ ìˆ˜ì§‘
        print("\n[1ë‹¨ê³„] ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        print("-" * 80)

        links = self.collect_links(start_url, crawl_rules)

        print(f"\nâœ… ì´ {len(links)}ê°œì˜ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

        # ë§í¬ ì €ì¥
        if save_links:
            links_file = os.path.join(self.output_dir, "collected_links.json")
            with open(links_file, "w", encoding="utf-8") as f:
                json.dump(links, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ ë§í¬ ëª©ë¡ ì €ì¥: {links_file}")

        # 2ë‹¨ê³„: ê° ë§í¬ í¬ë¡¤ë§ ë° êµ¬ì¡°í™”
        print("\n[2ë‹¨ê³„] ê° í˜ì´ì§€ í¬ë¡¤ë§ ë° LLM êµ¬ì¡°í™” ì¤‘...")
        print("-" * 80)

        structured_data_list = []
        failed_urls = []

        for idx, link_info in enumerate(links, 1):
            url = link_info["url"]
            name = link_info["name"]

            print(f"\n[{idx}/{len(links)}] ì²˜ë¦¬ ì¤‘: {name}")
            print(f"  URL: {url}")

            try:
                # í¬ë¡¤ë§ ë° êµ¬ì¡°í™”
                structured_data = self.crawler.crawl_and_structure(url=url)

                # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ ë§í¬ ì´ë¦„ìœ¼ë¡œ ì„¤ì •
                if not structured_data.title or structured_data.title.strip() == "":
                    structured_data.title = name

                # ìµœì¢… JSON êµ¬ì¡°ë¡œ ë³€í™˜
                final_data = {
                    "id": str(uuid.uuid4()),  # ê³ ìœ  ID ìë™ ìƒì„±
                    "title": structured_data.title,
                    "support_target": structured_data.eligibility,  # eligibility â†’ support_target
                    "support_content": structured_data.support,  # support â†’ support_content
                    "raw_text": structured_data.raw_text,
                    "source_url": url,
                    "region": self.region
                    or self.extract_region_from_url(url),  # ì§€ì—­ëª…
                }

                structured_data_list.append(final_data)
                print("  âœ… ì„±ê³µ")

            except Exception as e:
                print(f"  âŒ ì‹¤íŒ¨: {e}")
                failed_urls.append({"url": url, "name": name, "error": str(e)})

        # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        print("\n[3ë‹¨ê³„] ê²°ê³¼ ì €ì¥ ì¤‘...")
        print("-" * 80)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ì „ì²´ êµ¬ì¡°í™” ë°ì´í„° ì €ì¥
        output_file = os.path.join(self.output_dir, f"structured_data_{timestamp}.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(structured_data_list, f, ensure_ascii=False, indent=2)
        print(f"âœ… êµ¬ì¡°í™” ë°ì´í„° ì €ì¥: {output_file}")

        # ì‹¤íŒ¨í•œ URL ì €ì¥
        if failed_urls:
            failed_file = os.path.join(self.output_dir, f"failed_urls_{timestamp}.json")
            with open(failed_file, "w", encoding="utf-8") as f:
                json.dump(failed_urls, f, ensure_ascii=False, indent=2)
            print(f"âš ï¸  ì‹¤íŒ¨í•œ URL ì €ì¥: {failed_file}")

        # ìš”ì•½ ì •ë³´
        summary = {
            "timestamp": timestamp,
            "start_url": start_url,
            "total_links": len(links),
            "successful": len(structured_data_list),
            "failed": len(failed_urls),
            "output_file": output_file,
        }

        summary_file = os.path.join(self.output_dir, f"summary_{timestamp}.json")
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
        print("=" * 80)
        print(f"ğŸ“Š ì´ ë§í¬ ìˆ˜: {len(links)}")
        print(f"âœ… ì„±ê³µ: {len(structured_data_list)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
        print("=" * 80)

        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì›Œí¬í”Œë¡œìš°"
    )
    parser.add_argument("--url", type=str, help="ì‹œì‘ URL (ë³´ê±´ì†Œ ë³´ê±´ì‚¬ì—… í˜ì´ì§€)")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="output",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)",
    )
    parser.add_argument(
        "--region",
        type=str,
        default=None,
        help="ì§€ì—­ëª… (ì˜ˆ: ë™ì‘êµ¬). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ URLì—ì„œ ìë™ ì¶”ì¶œ",
    )

    args = parser.parse_args()

    # ëŒ€í™”í˜• ëª¨ë“œ
    if not args.url:
        print("\n" + "=" * 80)
        print("ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°")
        print("=" * 80)

        url = input("\nì‹œì‘ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not url:
            print("âŒ URLì„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        output_dir_input = input("ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (Enter: output): ").strip()
        output_dir = output_dir_input if output_dir_input else "output"

        # ì§€ì—­ëª…
        region_input = input("ì§€ì—­ëª… (Enter: URLì—ì„œ ìë™ ì¶”ì¶œ): ").strip()
        region = region_input if region_input else None

    else:
        url = args.url
        output_dir = args.output_dir
        region = args.region

    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    workflow = HealthCareWorkflow(output_dir=output_dir, region=region)

    try:
        summary = workflow.run(start_url=url)
        print("\nâœ… ì›Œí¬í”Œë¡œìš° ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()

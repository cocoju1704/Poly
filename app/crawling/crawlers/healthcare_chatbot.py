"""
í†µí•© í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡: ì›Œí¬í”Œë¡œìš° + Agentic RAG

1. workflow.py ê¸°ëŠ¥: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê±´ê°• ì§€ì› ì •ë³´ ìˆ˜ì§‘ ë° êµ¬ì¡°í™”
2. agent.py ê¸°ëŠ¥: FAISS ë²¡í„° ìŠ¤í† ì–´ + ê²€ìƒ‰ ë„êµ¬ + ë©€í‹°í„´ ëŒ€í™”
3. PDF ë¡œë”: PyMuPDFë¥¼ ì‚¬ìš©í•œ PDF íŒŒì¼ ì²˜ë¦¬
"""

import json
import os
import sys
import asyncio
from typing import List, Dict
from datetime import datetime

from workflow import HealthCareWorkflow

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.tools import tool
from langchain_community.vectorstores import FAISS
from langchain.agents import create_openai_tools_agent, AgentExecutor
from langchain_core.documents import Document

# crawler í´ë” import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "crawler"))


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
TEMP = float(os.getenv("TEMPERATURE", "0.2"))


class HealthCareChatbot:
    """í†µí•© í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡ - ë°ì´í„° ìˆ˜ì§‘ + RAG ê²€ìƒ‰ + ëŒ€í™”"""

    def __init__(
        self, output_dir: str = "output", data_file: str = None, region: str = None,
        chunk_strategy: str = "per_item"
    ):
        """
        Args:
            output_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            data_file: ê¸°ì¡´ JSON íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
            region: ì§€ì—­ëª… (ë°ì´í„° ìˆ˜ì§‘ ì‹œ ì‚¬ìš©)
        """
        self.output_dir = output_dir
        self.data_file = data_file
        self.region = region
        self.structured_data = []
        self.vector_store = None
        self.agent_executor = None
        self.conversation_region = None  # ëŒ€í™” ì‹œ ì‚¬ìš©í•  ì§€ì—­ëª…

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        # FAISS ì €ì¥ ê²½ë¡œ
        self.faiss_dir = os.path.join(self.output_dir, "faiss_index")
        # ì„ë² ë”© ëª¨ë¸ëª… ê³µìœ  (ì €ì¥/ë¡œë“œ ì‹œ ë™ì¼í•´ì•¼ í•¨)
        self.embedding_model_name = 'text-embedding-3-large'
        # ì²­í‚¹ ì „ëµ: per_item | by_fields | split
        self.chunk_strategy = chunk_strategy

    def collect_data(
        self,
        start_url: str,
        crawl_rules: List[Dict] = None,
        force_recollect: bool = False,
    ) -> str:
        """
        ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (workflow.py ê¸°ëŠ¥)

        Args:
            start_url: ì‹œì‘ URL
            crawl_rules: í¬ë¡¤ë§ ê·œì¹™
            force_recollect: Trueë©´ ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ì¬ìˆ˜ì§‘

        Returns:
            ìƒì„±ëœ JSON íŒŒì¼ ê²½ë¡œ
        """
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆê³  ì¬ìˆ˜ì§‘ ì•ˆ í•˜ë©´ ê±´ë„ˆë›°ê¸°
        if self.data_file and os.path.exists(self.data_file) and not force_recollect:
            print(f"âœ“ ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©: {self.data_file}")
            return self.data_file

        print("\n" + "=" * 80)
        print("ğŸ” ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (workflow)")
        print("=" * 80)

        # workflow ì‹¤í–‰
        workflow = HealthCareWorkflow(output_dir=self.output_dir, region=self.region)

        summary = workflow.run(start_url=start_url, crawl_rules=crawl_rules)

        self.data_file = summary["output_file"]
        print(f"\nâœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {self.data_file}")

        return self.data_file


    def load_data(self, data_file: str = None) -> List[Dict]:
        """
        JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ

        Args:
            data_file: JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ self.data_file ì‚¬ìš©)

        Returns:
            êµ¬ì¡°í™”ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        file_path = data_file or self.data_file

        if not file_path or not os.path.exists(file_path):
            raise FileNotFoundError(
                f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\n"
                "ë¨¼ì € collect_data() ë˜ëŠ” load_pdf()ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”."
            )

        print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            self.structured_data = json.load(f)

        print(f"âœ… {len(self.structured_data)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

        return self.structured_data

    def build_vector_store(self) -> FAISS:
        """
        FAISS ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• (agent.py ê¸°ëŠ¥)

        Returns:
            FAISS ë²¡í„° ìŠ¤í† ì–´
        """
        if not self.structured_data:
            raise ValueError("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € load_data()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

        print("\n" + "=" * 80)
        print("ğŸ§  ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘...")
        print("=" * 80)

        # ë¬¸ì„œ ìƒì„±
        documents = []
        for item in self.structured_data:
            raw_text = item.get("raw_text", "")
            if not raw_text:
                continue

            # ë©”íƒ€ë°ì´í„° í¬í•¨
            metadata = {
                "id": item.get("id", ""),
                "title": item.get("title", ""),
                "source_url": item.get("source_url", ""),
                "region": item.get("region", ""),
                "support_target": item.get("support_target", ""),
                "support_content": item.get("support_content", ""),
            }
            # PDF í˜ì´ì§€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if "page_number" in item:
                metadata["page_number"] = item.get("page_number")
                metadata["total_pages"] = item.get("total_pages")

            doc = Document(page_content=raw_text, metadata=metadata)
            documents.append(doc)

        print(f"  â†’ {len(documents)}ê°œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")

        
        # ê¸¸ì´ ê¸°ë°˜ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=700, chunk_overlap=100, separators=["\n\n", "\n", " ", ""]
        )
        chunks = splitter.split_documents(documents)
        print(f"  â†’ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ(split)")

        # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print("  â†’ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
        embeddings = OpenAIEmbeddings(model=self.embedding_model_name)
        
        self.vector_store = FAISS.from_documents(chunks, embeddings)
        # ë¡œì»¬ ì €ì¥
        os.makedirs(self.faiss_dir, exist_ok=True)
        self.vector_store.save_local(self.faiss_dir)

        print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ (ì €ì¥ ìœ„ì¹˜: {self.faiss_dir})\n")

        return self.vector_store

    def load_vector_store(self) -> FAISS:
        """
        ë¡œì»¬ì— ì €ì¥ëœ FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ

        Returns:
            FAISS ë²¡í„° ìŠ¤í† ì–´ (ì„±ê³µ ì‹œ), ì—†ìœ¼ë©´ None
        """
        # ì €ì¥ íŒŒì¼ ì¡´ì¬ í™•ì¸ (ê¸°ë³¸ íŒŒì¼: index.faiss, index.pkl)
        if not os.path.isdir(self.faiss_dir):
            return None
        index_faiss = os.path.join(self.faiss_dir, "index.faiss")
        index_pkl = os.path.join(self.faiss_dir, "index.pkl")
        if not (os.path.exists(index_faiss) and os.path.exists(index_pkl)):
            return None

        print("\n" + "=" * 80)
        print("ğŸ“¦ ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        print("=" * 80)

        embeddings = OpenAIEmbeddings(model=self.embedding_model_name)

        try:
            # allow_dangerous_deserialization ì€ ìµœì‹  ë²„ì „ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆìŒ
            self.vector_store = FAISS.load_local(
                self.faiss_dir, embeddings, allow_dangerous_deserialization=True
            )
            print("âœ… ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ\n")
            return self.vector_store
        except Exception as e:
            print(f"âš ï¸  ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def setup_agent(self):
        """
        LangChain ì—ì´ì „íŠ¸ ì„¤ì • (agent.py ê¸°ëŠ¥)
        """
        if not self.vector_store:
            raise ValueError(
                "ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vector_store()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
            )

        print("ğŸ¤– ì—ì´ì „íŠ¸ ì„¤ì • ì¤‘...")

        @tool
        def search_with_score(query: str) -> str:
            """
            ê±´ê°• ì§€ì› ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            """
            try:
                results = self.vector_store.similarity_search_with_score(
                    query, 
                    k=7
                )

                if not results:
                    return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

                out = []
                for i, (doc, score) in enumerate(results, start=1):
                    meta = doc.metadata
                    preview = doc.page_content[:200].replace("\n", " ")

                    out.append(
                        f"[ë¬¸ì„œ {i} | ì ìˆ˜: {score:.4f}]\n"
                        f"ì œëª©: {meta.get('title', 'N/A')}\n"
                        f"ì§€ì—­: {meta.get('region', 'N/A')}\n"
                        f"ë‚´ìš©: {preview}...\n"
                        f"URL: {meta.get('source_url', 'N/A')}\n"
                    )

                return "\n".join(out)
            except Exception as e:
                return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

        tools = [search_with_score]

        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë³´ê±´ì†Œ ê±´ê°• ì§€ì› ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ ìƒë‹´ì›ì…ë‹ˆë‹¤.

ì§€ì¹¨:
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ê²ƒ
- ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•  ê²ƒ
- ì§€ì› ëŒ€ìƒ, ì§€ì› ë‚´ìš©, ì‹ ì²­ ë°©ë²• ë“± í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•  ê²ƒ
- ì—¬ëŸ¬ ì§€ì—­ì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ì§€ì—­ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì•ˆë‚´í•´ì•¼í•˜ë©° ë§Œì•½ ì œê³µëœ ë¬¸ì„œì— ì„¸ë¶€ ì§€ì› ë‚´ìš©ì´ ì¡´ì¬í•œë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•  ê²ƒ
- ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§íˆ ë‹µë³€í•  ê²ƒ
- ì˜ˆì‹œë¡œ ì§ˆë¬¸ : ì•” ì§€ì›ì— ëŒ€í•´ ì•Œë ¤ì¤˜ ì¸ ê²½ìš° ì œê³µ ë¬¸ì„œì— ì•” ì§€ì›ì´ ì—†ìœ¼ë©´ ì°¸ì¡° í•˜ì§€ ì•Šì„ ê²ƒ
- ë‹µë³€ ëì—ëŠ” ì¶œì²˜ URLì„ ì œê³µí•˜ì„¸ìš”.
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", SYSTEM_PROMPT),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ]
        )

        # LLM ë° ì—ì´ì „íŠ¸ ìƒì„±
        llm = ChatOpenAI(model=MODEL, temperature=TEMP, streaming=True)
        agent = create_openai_tools_agent(llm, tools, prompt)

        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=5,
        )

        print("âœ… ì—ì´ì „íŠ¸ ì„¤ì • ì™„ë£Œ\n")

    def print_summary(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½ ì¶œë ¥"""
        if not self.structured_data:
            print("âš ï¸  ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("\n" + "=" * 80)
        print("ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½")
        print("=" * 80)

        # ì§€ì—­ë³„ í†µê³„
        region_count = {}
        for item in self.structured_data:
            region = item.get("region", "ë¯¸ì§€ì •")
            region_count[region] = region_count.get(region, 0) + 1

        print(f"\nì´ ë¬¸ì„œ ìˆ˜: {len(self.structured_data)}ê°œ")
        print("\nì§€ì—­ë³„ ë¶„í¬:")
        for region, count in region_count.items():
            print(f"  - {region}: {count}ê°œ")

        print("\nìµœê·¼ ë¬¸ì„œ 3ê°œ:")
        for i, item in enumerate(self.structured_data[:3], 1):
            print(f"\n  [{i}] {item.get('title', 'N/A')}")
            print(f"      ì§€ì—­: {item.get('region', 'N/A')}")
            print(f"      URL: {item.get('source_url', 'N/A')}")

        print("\n" + "=" * 80)

    async def run_conversation(self):
        """
        ë©€í‹°í„´ ëŒ€í™” ì‹¤í–‰ (agent.py ê¸°ëŠ¥)
        """
        if not self.agent_executor:
            raise ValueError(
                "ì—ì´ì „íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € setup_agent()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
            )

        chat_history = []

        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        self.print_summary()

        print("\n" + "=" * 80)
        print("ğŸ’¬ í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡ (ê±´ê°• ì§€ì› ì •ë³´ ìƒë‹´)")
        print("=" * 80)
        print("ì¢…ë£Œ: quit/exit/ì¢…ë£Œ | ì´ˆê¸°í™”: reset/clear/ì´ˆê¸°í™”")
        print("=" * 80)

        while True:
            user_input = await asyncio.to_thread(input, "ì¢…ë£Œë¥¼ ì›í•˜ì‹œë©´ ì¢…ë£Œ/exit/quit ì…ë ¥\nì´ˆê¸°í™”ë¥¼ ì›í•˜ì‹œë©´ ì´ˆê¸°í™”/reset/clear ì…ë ¥\nì§ˆë¬¸: ")
            if user_input is None:
                continue
            user_input = user_input.strip()

            # ì¢…ë£Œ ëª…ë ¹
            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            # ì´ˆê¸°í™” ëª…ë ¹
            if user_input.lower() in ["reset", "clear", "ì´ˆê¸°í™”"]:
                chat_history = []
                print("\nğŸ”„ ëŒ€í™” ë‚´ìš©ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                self.print_summary()
                continue

            if not user_input:
                continue

            try:
                print("ë‹µë³€: ", end="", flush=True)
                full_response = ""

                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                async for event in self.agent_executor.astream_events(
                    {"input": user_input, "chat_history": chat_history},
                    version="v2",
                ):
                    kind = event["event"]

                    if kind == "on_tool_start":
                        tool_name = event["name"]
                        print(f"\n[ğŸ” {tool_name} ê²€ìƒ‰ ì¤‘...]", end="", flush=True)
                        print("\në‹µë³€: ", end="", flush=True)

                    elif kind == "on_chat_model_stream":
                        chunk = event["data"]["chunk"].content
                        if chunk:
                            print(chunk, end="", flush=True)
                            full_response += chunk

                print()  # ì¤„ë°”ê¿ˆ

                # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
                chat_history.append(HumanMessage(content=user_input))
                chat_history.append(AIMessage(content=full_response))

            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def initialize(
        self,
        start_url: str = None,
        data_file: str = None,
        pdf_file: str = None,
        crawl_rules: List[Dict] = None,
        force_recollect: bool = False,
    ):
        """
        ì±—ë´‡ ì´ˆê¸°í™” (ì „ì²´ íŒŒì´í”„ë¼ì¸)

        Args:
            start_url: ë°ì´í„° ìˆ˜ì§‘í•  URL (data_file, pdf_fileì´ ì—†ì„ ë•Œ í•„ìš”)
            data_file: ê¸°ì¡´ JSON íŒŒì¼ ê²½ë¡œ
            pdf_file: PDF íŒŒì¼ ê²½ë¡œ
            crawl_rules: í¬ë¡¤ë§ ê·œì¹™
            force_recollect: ê°•ì œ ì¬ìˆ˜ì§‘ ì—¬ë¶€
        """
        print("\n" + "=" * 80)
        print("ğŸš€ í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡ ì´ˆê¸°í™”")
        print("=" * 80)

        # 1. ë°ì´í„° ì¤€ë¹„
        if pdf_file:
            print(f"\n[1] PDF íŒŒì¼ ì²˜ë¦¬: {pdf_file}")
            self.load_pdf(pdf_file, force_recollect)
            # PDF ì²˜ë¦¬ í›„ ìë™ìœ¼ë¡œ ë°ì´í„°ê°€ ë¡œë“œë¨ (self.structured_data)
        elif data_file and os.path.exists(data_file):
            print(f"\n[1] ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©: {data_file}")
            self.data_file = data_file
        elif start_url:
            print(f"\n[1] ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘: {start_url}")
            self.collect_data(start_url, crawl_rules, force_recollect)
        else:
            raise ValueError(
                "start_url, data_file, pdf_file ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.\n"
                "ìƒˆë¡œ ìˆ˜ì§‘í•˜ë ¤ë©´ start_urlì„, ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ data_fileì„, "
                "PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´ pdf_fileì„ ì œê³µí•˜ì„¸ìš”."
            )

        # 2. ë°ì´í„° ë¡œë“œ (PDFê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
        if not pdf_file:
            print("\n[2] ë°ì´í„° ë¡œë“œ")
            self.load_data()
        else:
            print("\n[2] PDF ë°ì´í„° ì´ë¯¸ ë¡œë“œë¨")

        # 3. ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ/êµ¬ì¶•
        print("\n[3] ë²¡í„° ìŠ¤í† ì–´ ì¤€ë¹„")
        loaded = self.load_vector_store()
        if loaded is None:
            print("ë¡œì»¬ ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
            self.build_vector_store()

        # 4. ì—ì´ì „íŠ¸ ì„¤ì •
        print("\n[4] ì—ì´ì „íŠ¸ ì„¤ì •")
        self.setup_agent()

        print("\n" + "=" * 80)
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("=" * 80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="í†µí•© í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡ - ë°ì´í„° ìˆ˜ì§‘ + RAG + ëŒ€í™”"
    )
    parser.add_argument("--url", type=str, help="ë°ì´í„° ìˆ˜ì§‘í•  ì›¹ì‚¬ì´íŠ¸ URL")
    parser.add_argument("--data-file", type=str, help="ê¸°ì¡´ JSON ë°ì´í„° íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--pdf-file", type=str, help="PDF íŒŒì¼ ê²½ë¡œ (PyMuPDF ì‚¬ìš©)")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="output",
        help="ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)",
    )
    parser.add_argument("--region", type=str, help="ì§€ì—­ëª… (ì˜ˆ: ë™ì‘êµ¬)")
    parser.add_argument(
        "--force-recollect",
        action="store_true",
        help="ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ê°•ì œ ì¬ìˆ˜ì§‘",
    )
    parser.add_argument(
        "--chunk-strategy",
        type=str,
        choices=["per_item", "by_fields", "split"],
        default="per_item",
        help="ì²­í‚¹ ì „ëµ ì„ íƒ (per_item | by_fields | split)",
    )

    args = parser.parse_args()

    # ëŒ€í™”í˜• ëª¨ë“œ
    if not args.url and not args.data_file and not args.pdf_file:
        print("\n" + "=" * 80)
        print("í†µí•© í—¬ìŠ¤ì¼€ì–´ ì±—ë´‡")
        print("=" * 80)
        print("\në°ì´í„° ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("  1. ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìƒˆë¡œ ìˆ˜ì§‘")
        print("  2. ê¸°ì¡´ JSON íŒŒì¼ ì‚¬ìš©")
        print("  3. PDF íŒŒì¼ ì‚¬ìš© ## í˜„ì¬ ì‚¬ìš© ì•ˆí•¨ ##")

        choice = input("\nì„ íƒ (1, 2, ë˜ëŠ” 3): ").strip()

        if choice == "1":
            url = input("ì›¹ì‚¬ì´íŠ¸ URL: ").strip()
            if not url:
                print("âŒ URLì„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            region = input("ì§€ì—­ëª… (Enter: ìë™ ì¶”ì¶œ): ").strip() or None
            data_file = None
            pdf_file = None
        elif choice == "2":
            data_file = input("JSON íŒŒì¼ ê²½ë¡œ: ").strip()
            if not data_file or not os.path.exists(data_file):
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
                return
            url = None
            pdf_file = None
            region = None
        elif choice == "3":
            pdf_file = input("PDF íŒŒì¼ ê²½ë¡œ: ").strip()
            if not pdf_file or not os.path.exists(pdf_file):
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}")
                return
            url = None
            data_file = None
            region = input("ì§€ì—­ëª… (Enter: ë¯¸ì§€ì •): ").strip() or None
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return

        output_dir = input("ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (Enter: output): ").strip() or "output"

    else:
        url = args.url
        data_file = args.data_file
        pdf_file = args.pdf_file
        output_dir = args.output_dir
        region = args.region

    # ì±—ë´‡ ìƒì„± ë° ì´ˆê¸°í™”
    try:
        chatbot = HealthCareChatbot(
            output_dir=output_dir, data_file=data_file, region=region,
            chunk_strategy=getattr(args, "chunk_strategy", "per_item")
        )

        chatbot.initialize(
            start_url=url,
            data_file=data_file,
            pdf_file=pdf_file,
            force_recollect=args.force_recollect
            if hasattr(args, "force_recollect")
            else False,
        )

        # ëŒ€í™” ì‹œì‘
        asyncio.run(chatbot.run_conversation())

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
